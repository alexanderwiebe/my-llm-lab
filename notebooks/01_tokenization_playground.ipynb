{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefdee1e",
   "metadata": {},
   "source": [
    "# Tokenization Playground\n",
    "\n",
    "This notebook explores different tokenization techniques used in NLP and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44f5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43614ea0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64acf32c",
   "metadata": {},
   "source": [
    "## OpenAI Tokenization with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89c752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! This is a sample text for tokenization.\n",
      "Tokens: [9906, 11, 1917, 0, 1115, 374, 264, 6205, 1495, 369, 4037, 2065, 13]\n",
      "justHello tokens: [9906]\n",
      "Number of tokens: 13\n",
      "Decoded text: Hello, world! This is a sample text for tokenization.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI tokenizer\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 encoding\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a sample text for tokenization.\"\n",
    "justHello = \"Hello\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = encoding.encode(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"justHello tokens: {encoding.encode(justHello)}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = encoding.decode(tokens)\n",
    "print(f\"Decoded text: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f0236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case sensitivity test:\n",
      "'hello' -> [15339]\n",
      "'Hello' -> [9906]\n",
      "'HELLO' -> [51812, 1623]\n",
      "\n",
      "First token of 'HELLO': 51812\n",
      "Decoded: HEL\n",
      "With leading space:\n",
      "' hello' -> [24748]\n",
      "' Hello' -> [22691]\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to test case sensitivity\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Test case sensitivity\n",
    "hello_lower = \"hello\"\n",
    "hello_upper = \"Hello\" \n",
    "hello_caps = \"HELLO\"\n",
    "\n",
    "tokens_lower = encoding.encode(hello_lower)\n",
    "tokens_upper = encoding.encode(hello_upper)\n",
    "tokens_caps = encoding.encode(hello_caps)\n",
    "\n",
    "print(\"Case sensitivity test:\")\n",
    "print(f\"'hello' -> {tokens_lower}\")\n",
    "print(f\"'Hello' -> {tokens_upper}\")\n",
    "print(f\"'HELLO' -> {tokens_caps}\")\n",
    "print()\n",
    "# Only pass the first token of tokens_caps to decode\n",
    "print(f\"First token of 'HELLO': {tokens_caps[0]}\")\n",
    "print(f\"Decoded: {encoding.decode([tokens_caps[0]])}\")\n",
    "# Test with spaces\n",
    "space_hello = \" hello\"\n",
    "space_Hello = \" Hello\"\n",
    "\n",
    "print(\"With leading space:\")\n",
    "print(f\"' hello' -> {encoding.encode(space_hello)}\")\n",
    "print(f\"' Hello' -> {encoding.encode(space_Hello)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755836b",
   "metadata": {},
   "source": [
    "## What is cl100k_base?\n",
    "\n",
    "`cl100k_base` is a **tokenizer encoding** (vocabulary), not an LLM itself. Think of it as the \"dictionary\" that tells the model how to convert text into numbers.\n",
    "\n",
    "**Different OpenAI models use different encodings:**\n",
    "- `cl100k_base`: Used by GPT-4, GPT-3.5-turbo, text-embedding-ada-002\n",
    "- `p50k_base`: Used by Codex models, text-davinci-002, text-davinci-003\n",
    "- `r50k_base`: Used by older GPT-3 models (davinci, curie, babbage, ada)\n",
    "\n",
    "The encoding determines:\n",
    "- How text gets split into tokens\n",
    "- What the vocabulary size is (cl100k_base has ~100k tokens)\n",
    "- How efficiently different types of text are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffa487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing encodings for the same text:\n",
      "Text: 'Hello, world! This is a sample text for tokenization.'\n",
      "\n",
      "cl100k_base (GPT-4, GPT-3.5-turbo):\n",
      "  Tokens: 13\n",
      "  Token IDs: [9906, 11, 1917, 0, 1115, 374, 264, 6205, 1495, 369]...\n",
      "\n",
      "p50k_base (Codex, text-davinci-002/003):\n",
      "  Tokens: 13\n",
      "  Token IDs: [15496, 11, 995, 0, 770, 318, 257, 6291, 2420, 329]...\n",
      "\n",
      "r50k_base (GPT-3 davinci, curie, babbage, ada):\n",
      "  Tokens: 13\n",
      "  Token IDs: [15496, 11, 995, 0, 770, 318, 257, 6291, 2420, 329]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different OpenAI encodings\n",
    "import tiktoken\n",
    "\n",
    "text = \"Hello, world! This is a sample text for tokenization.\"\n",
    "\n",
    "# Different encodings used by different models\n",
    "encodings = {\n",
    "    \"cl100k_base\": \"GPT-4, GPT-3.5-turbo\",\n",
    "    \"p50k_base\": \"Codex, text-davinci-002/003\", \n",
    "    \"r50k_base\": \"GPT-3 davinci, curie, babbage, ada\"\n",
    "}\n",
    "\n",
    "print(\"Comparing encodings for the same text:\")\n",
    "print(f\"Text: '{text}'\")\n",
    "print()\n",
    "\n",
    "for encoding_name, models in encodings.items():\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        tokens = encoding.encode(text)\n",
    "        print(f\"{encoding_name} ({models}):\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Token IDs: {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"{encoding_name}: Error - {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e724d",
   "metadata": {},
   "source": [
    "## Is cl100k_base Open Source?\n",
    "\n",
    "**Yes and No** - it's complicated:\n",
    "\n",
    "### What's Available:\n",
    "- ✅ **Tokenizer implementation**: The `tiktoken` library is open source\n",
    "- ✅ **Encoding rules**: You can encode/decode text freely\n",
    "- ✅ **Vocabulary mappings**: The token-to-text mappings are accessible\n",
    "- ✅ **Usage**: No restrictions on using it in your projects\n",
    "\n",
    "### What's NOT Available:\n",
    "- ❌ **Training details**: How exactly it was trained isn't fully documented\n",
    "- ❌ **Training data**: The specific dataset used to create the vocabulary\n",
    "- ❌ **Full methodology**: Complete details of the BPE training process\n",
    "\n",
    "### Practical Impact:\n",
    "- You can **use** cl100k_base freely in any project\n",
    "- You can **inspect** what tokens exist and how text gets tokenized\n",
    "- You **cannot** easily recreate or modify the encoding from scratch\n",
    "\n",
    "It's \"open source\" in terms of usage, but not in terms of reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b63f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring what's accessible about cl100k_base\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"What we CAN access about cl100k_base:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Vocabulary size\n",
    "print(f\"Vocabulary size: {encoding.n_vocab:,} tokens\")\n",
    "\n",
    "# 2. Some example tokens and their IDs\n",
    "print(f\"\\nSample token mappings:\")\n",
    "test_words = [\"hello\", \"world\", \"Python\", \"AI\", \"🤖\", \" the\"]\n",
    "for word in test_words:\n",
    "    tokens = encoding.encode(word)\n",
    "    print(f\"  '{word}' -> {tokens} -> '{encoding.decode(tokens)}'\")\n",
    "\n",
    "# 3. Special tokens (if any)\n",
    "try:\n",
    "    special_tokens = encoding.special_tokens_set\n",
    "    print(f\"\\nSpecial tokens: {special_tokens}\")\n",
    "except:\n",
    "    print(f\"\\nSpecial tokens: Not directly accessible\")\n",
    "\n",
    "# 4. Max token value\n",
    "sample_tokens = [encoding.encode(word)[0] for word in [\"a\", \"the\", \"and\", \"hello\", \"world\"]]\n",
    "print(f\"\\nSample token IDs: {sample_tokens}\")\n",
    "print(f\"Max token ID we can easily find: {max(sample_tokens)}\")\n",
    "\n",
    "print(f\"\\nWhat we CANNOT easily access:\")\n",
    "print(\"- The original training dataset\")\n",
    "print(\"- Exact BPE merge rules\") \n",
    "print(\"- Training hyperparameters\")\n",
    "print(\"- Step-by-step creation process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957b32c",
   "metadata": {},
   "source": [
    "## Is Tokenization Just Dictionary Lookup?\n",
    "\n",
    "**Mostly yes, but with some important nuances:**\n",
    "\n",
    "### The Simple View (Encoding/Decoding):\n",
    "- ✅ **Encoding**: Text → Look up in dictionary → Token IDs\n",
    "- ✅ **Decoding**: Token IDs → Look up in dictionary → Text  \n",
    "- This part is indeed just dictionary lookup!\n",
    "\n",
    "### The Complex Part (How the Dictionary Was Built):\n",
    "The \"dictionary\" itself was created through a sophisticated process:\n",
    "\n",
    "1. **Byte Pair Encoding (BPE)**: Iteratively merges the most frequent character pairs\n",
    "2. **Frequency Analysis**: Analyzes massive text corpora to find optimal splits\n",
    "3. **Optimization**: Balances vocabulary size vs. text compression efficiency\n",
    "\n",
    "### Key Insight:\n",
    "- **Using** the tokenizer = Simple dictionary lookup ✅\n",
    "- **Creating** the tokenizer = Complex machine learning process 🧠\n",
    "\n",
    "Think of it like using Google Translate vs. building Google Translate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48353d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ENCODING (Text → Token IDs) - Dictionary Lookup:\n",
      "============================================================\n",
      "Input: 'Hello, AI researcher!'\n",
      "Output: [9906, 11, 15592, 32185, 0]\n",
      "\n",
      "Breaking it down piece by piece:\n",
      "  Token 1: 'Hello' → 9906\n",
      "  Token 2: ',' → 11\n",
      "  Token 3: ' AI' → 15592\n",
      "  Token 4: ' researcher' → 32185\n",
      "  Token 5: '!' → 0\n",
      "\n",
      "🔍 DECODING (Token IDs → Text) - Reverse Dictionary Lookup:\n",
      "============================================================\n",
      "Input: [9906, 11, 15592, 32185, 0]\n",
      "Output: 'Hello, AI researcher!'\n",
      "\n",
      "Breaking it down piece by piece:\n",
      "  Token ID 9906 → 'Hello'\n",
      "  Token ID 11 → ','\n",
      "  Token ID 15592 → ' AI'\n",
      "  Token ID 32185 → ' researcher'\n",
      "  Token ID 0 → '!'\n",
      "\n",
      "🧠 THE COMPLEX PART (Creating the Dictionary):\n",
      "============================================================\n",
      "❓ Why does 'Hello' get token 9906?\n",
      "❓ Why does ',' get token 11?\n",
      "❓ Why does ' AI' (with space) get token 15592?\n",
      "\n",
      "👆 THESE decisions came from analyzing billions of text examples\n",
      "   to find the most efficient way to split language into pieces!\n",
      "\n",
      "🎯 Example of BPE intelligence:\n",
      "'antidisestablishmentarianism' →\n",
      "  'ant' (519)\n",
      "  'idis' (85342)\n",
      "  'establish' (34500)\n",
      "  'ment' (479)\n",
      "  'arian' (8997)\n",
      "  'ism' (2191)\n",
      "👆 The tokenizer never saw this exact word, but intelligently splits it!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating the \"dictionary lookup\" nature of tokenization\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"🔍 ENCODING (Text → Token IDs) - Dictionary Lookup:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "text = \"Hello, AI researcher!\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"Input: '{text}'\")\n",
    "print(f\"Output: {tokens}\")\n",
    "print()\n",
    "\n",
    "# Show it's just mapping each piece to a number\n",
    "print(\"Breaking it down piece by piece:\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    # Decode each individual token to see what text it represents\n",
    "    piece = encoding.decode([token_id])\n",
    "    print(f\"  Token {i+1}: '{piece}' → {token_id}\")\n",
    "\n",
    "print(\"\\n\" + \"🔍 DECODING (Token IDs → Text) - Reverse Dictionary Lookup:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Input: {tokens}\")\n",
    "decoded = encoding.decode(tokens)\n",
    "print(f\"Output: '{decoded}'\")\n",
    "print()\n",
    "\n",
    "print(\"Breaking it down piece by piece:\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    piece = encoding.decode([token_id])\n",
    "    print(f\"  Token ID {token_id} → '{piece}'\")\n",
    "\n",
    "print(\"\\n\" + \"🧠 THE COMPLEX PART (Creating the Dictionary):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"❓ Why does 'Hello' get token 9906?\")\n",
    "print(\"❓ Why does ',' get token 11?\") \n",
    "print(\"❓ Why does ' AI' (with space) get token 15592?\")\n",
    "print()\n",
    "print(\"👆 THESE decisions came from analyzing billions of text examples\")\n",
    "print(\"   to find the most efficient way to split language into pieces!\")\n",
    "\n",
    "# Show that some words get split in surprising ways\n",
    "print(f\"\\n🎯 Example of BPE intelligence:\")\n",
    "weird_word = \"antidisestablishmentarianism\"\n",
    "weird_tokens = encoding.encode(weird_word)\n",
    "print(f\"'{weird_word}' →\")\n",
    "for token_id in weird_tokens:\n",
    "    piece = encoding.decode([token_id])\n",
    "    print(f\"  '{piece}' ({token_id})\")\n",
    "print(\"👆 The tokenizer never saw this exact word, but intelligently splits it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca094368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 STEP-BY-STEP BPE BREAKDOWN\n",
      "==================================================\n",
      "Word: 'antidisestablishmentarianism'\n",
      "Final tokens: [519, 85342, 34500, 479, 8997, 2191]\n",
      "\n",
      "🎯 How BPE likely processed this word:\n",
      "(Note: This is a reconstruction - the actual BPE training was more complex)\n",
      "\n",
      "Step 1 - Start with characters:\n",
      "  ['a', 'n', 't', 'i', 'd', 'i', 's', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'a', 'n', 'i', 's', 'm']\n",
      "\n",
      "Step 2 - BPE iteratively merges frequent pairs:\n",
      "  (Showing the final result of many merge operations)\n",
      "\n",
      "Step 3 - Final tokenization:\n",
      "  Token 1: 'ant' (ID: 519)\n",
      "  Token 2: 'idis' (ID: 85342)\n",
      "  Token 3: 'establish' (ID: 34500)\n",
      "  Token 4: 'ment' (ID: 479)\n",
      "  Token 5: 'arian' (ID: 8997)\n",
      "  Token 6: 'ism' (ID: 2191)\n",
      "\n",
      "🧠 Why these specific splits?\n",
      "Each piece likely appeared frequently in the training data:\n",
      "  • 'ant' - common prefix (antenna, anticipate, etc.)\n",
      "  • 'idis' - learned as a unit from words like 'antidiscriminatory'\n",
      "  • 'establish' - very common word, gets its own token\n",
      "  • 'ment' - common suffix (development, movement, etc.)\n",
      "  • 'arian' - common in words like 'vegetarian', 'librarian'\n",
      "  • 'ism' - common suffix (capitalism, racism, etc.)\n",
      "\n",
      "👆 The algorithm learned these patterns from seeing millions of examples!\n",
      "\n",
      "🔍 Testing related words to see BPE patterns:\n",
      "'establish' → 'establish'\n",
      "'establishment' → 'establish' + 'ment'\n",
      "'vegetarian' → 'veget' + 'arian'\n",
      "'capitalism' → 'capital' + 'ism'\n"
     ]
    }
   ],
   "source": [
    "# Step-by-step BPE breakdown of \"antidisestablishmentarianism\"\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "word = \"antidisestablishmentarianism\"\n",
    "tokens = encoding.encode(word)\n",
    "\n",
    "print(\"🔬 STEP-BY-STEP BPE BREAKDOWN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Word: '{word}'\")\n",
    "print(f\"Final tokens: {tokens}\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 How BPE likely processed this word:\")\n",
    "print(\"(Note: This is a reconstruction - the actual BPE training was more complex)\")\n",
    "print()\n",
    "\n",
    "# Start with character level\n",
    "print(\"Step 1 - Start with characters:\")\n",
    "chars = list(word)\n",
    "print(f\"  {chars}\")\n",
    "print()\n",
    "\n",
    "print(\"Step 2 - BPE iteratively merges frequent pairs:\")\n",
    "print(\"  (Showing the final result of many merge operations)\")\n",
    "print()\n",
    "\n",
    "# Show the actual breakdown\n",
    "print(\"Step 3 - Final tokenization:\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    piece = encoding.decode([token_id])\n",
    "    print(f\"  Token {i+1}: '{piece}' (ID: {token_id})\")\n",
    "\n",
    "print()\n",
    "print(\"🧠 Why these specific splits?\")\n",
    "print(\"Each piece likely appeared frequently in the training data:\")\n",
    "print(\"  • 'ant' - common prefix (antenna, anticipate, etc.)\")\n",
    "print(\"  • 'idis' - learned as a unit from words like 'antidiscriminatory'\")  \n",
    "print(\"  • 'establish' - very common word, gets its own token\")\n",
    "print(\"  • 'ment' - common suffix (development, movement, etc.)\")\n",
    "print(\"  • 'arian' - common in words like 'vegetarian', 'librarian'\")\n",
    "print(\"  • 'ism' - common suffix (capitalism, racism, etc.)\")\n",
    "print()\n",
    "print(\"👆 The algorithm learned these patterns from seeing millions of examples!\")\n",
    "\n",
    "# Let's also test some related words to see patterns\n",
    "print(\"\\n🔍 Testing related words to see BPE patterns:\")\n",
    "related_words = [\"establish\", \"establishment\", \"vegetarian\", \"capitalism\"]\n",
    "for test_word in related_words:\n",
    "    test_tokens = encoding.encode(test_word)\n",
    "    print(f\"'{test_word}' → \", end=\"\")\n",
    "    for token_id in test_tokens:\n",
    "        piece = encoding.decode([token_id])\n",
    "        print(f\"'{piece}'\", end=\" + \" if token_id != test_tokens[-1] else \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1c894",
   "metadata": {},
   "source": [
    "## How BPE Training Actually Works (Conceptually)\n",
    "\n",
    "The BPE algorithm that created this \"dictionary\" worked like this:\n",
    "\n",
    "### Phase 1: Character Level Start\n",
    "```\n",
    "antidisestablishmentarianism → ['a','n','t','i','d','i','s','e','s','t',...]\n",
    "```\n",
    "\n",
    "### Phase 2: Iterative Merging (Thousands of Steps)\n",
    "The algorithm analyzed massive text corpora and repeatedly:\n",
    "\n",
    "1. **Count all adjacent pairs**: \"an\", \"nt\", \"ti\", \"id\", etc.\n",
    "2. **Find the most frequent pair**: Maybe \"th\" appears 1M times across all text\n",
    "3. **Merge that pair**: Replace all \"t\" + \"h\" with a single \"th\" token\n",
    "4. **Repeat**: Now \"th\" can form new pairs like \"the\", \"thi\", etc.\n",
    "\n",
    "### Phase 3: Result\n",
    "After thousands of iterations, common patterns emerge:\n",
    "- **\"establish\"** became one token (very frequent word)\n",
    "- **\"ment\"** became one token (frequent suffix)  \n",
    "- **\"ism\"** became one token (frequent suffix)\n",
    "- **\"ant\"** became one token (frequent prefix)\n",
    "\n",
    "### The Magic\n",
    "Even though the algorithm never saw \"antidisestablishmentarianism\" during training, it learned the building blocks that make it up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
