{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e44adb1",
   "metadata": {},
   "source": [
    "# Embeddings Playground\n",
    "\n",
    "This notebook explores how tokens (from tokenization) get converted into embeddings - fixed-length numerical vectors that capture semantic meaning.\n",
    "\n",
    "## Key Concepts:\n",
    "- **Tokens** → **Embeddings** → **Model Processing**\n",
    "- Embeddings are dense vector representations of tokens\n",
    "- Similar tokens have similar embeddings (in vector space)\n",
    "- Fixed dimensionality regardless of input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# For visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c8166",
   "metadata": {},
   "source": [
    "## From Tokens to Embeddings: The Journey\n",
    "\n",
    "Let's trace how text becomes embeddings:\n",
    "\n",
    "1. **Text** → \"Hello world\" \n",
    "2. **Tokenization** → [9906, 1917] (from notebook 01)\n",
    "3. **Embedding Lookup** → Each token gets a fixed-size vector\n",
    "4. **Result** → Matrix of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize text (connecting to notebook 01)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello world\",\n",
    "    \"Hello universe\", \n",
    "    \"Goodbye world\",\n",
    "    \"Python programming\",\n",
    "    \"Machine learning\"\n",
    "]\n",
    "\n",
    "print(\"🔤 TOKENIZATION STEP:\")\n",
    "print(\"=\" * 40)\n",
    "for text in texts:\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"'{text}' → {tokens}\")\n",
    "    \n",
    "    # Show the actual token pieces\n",
    "    pieces = []\n",
    "    for token in tokens:\n",
    "        piece = encoding.decode([token])\n",
    "        pieces.append(f\"'{piece}'\")\n",
    "    print(f\"  Token pieces: {' + '.join(pieces)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67dc319",
   "metadata": {},
   "source": [
    "## What Are Embeddings?\n",
    "\n",
    "**Embeddings** are dense numerical vectors that represent tokens in a high-dimensional space where:\n",
    "- Similar meanings → Similar vectors\n",
    "- Each token has a **fixed-length vector** (e.g., 1536 dimensions for OpenAI's text-embedding-ada-002)\n",
    "- Mathematical operations can capture semantic relationships\n",
    "\n",
    "### Conceptual Example:\n",
    "If embeddings were 3D instead of 1536D:\n",
    "- \"king\" → [0.2, 0.8, 0.1]\n",
    "- \"queen\" → [0.3, 0.9, 0.2] \n",
    "- \"cat\" → [0.7, 0.1, 0.9]\n",
    "\n",
    "Notice: \"king\" and \"queen\" are closer to each other than to \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb352f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate what embeddings look like conceptually\n",
    "# (This is just for illustration - real embeddings are much more complex)\n",
    "\n",
    "# Create some fake embeddings for demonstration\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Simulate 5-dimensional embeddings for some tokens\n",
    "token_embeddings = {\n",
    "    \"Hello\": np.random.normal(0.5, 0.1, 5),    # Greeting cluster\n",
    "    \"Hi\": np.random.normal(0.5, 0.1, 5),       # Greeting cluster  \n",
    "    \"Goodbye\": np.random.normal(-0.5, 0.1, 5), # Farewell cluster\n",
    "    \"Bye\": np.random.normal(-0.5, 0.1, 5),     # Farewell cluster\n",
    "    \"world\": np.random.normal(0.0, 0.1, 5),    # Noun cluster\n",
    "    \"universe\": np.random.normal(0.0, 0.1, 5), # Noun cluster\n",
    "    \"Python\": np.random.normal(0.8, 0.1, 5),   # Programming cluster\n",
    "    \"programming\": np.random.normal(0.8, 0.1, 5) # Programming cluster\n",
    "}\n",
    "\n",
    "print(\"🧮 SIMULATED TOKEN EMBEDDINGS (5D):\")\n",
    "print(\"=\" * 50)\n",
    "for token, embedding in token_embeddings.items():\n",
    "    print(f\"'{token}': [{', '.join(f'{x:.2f}' for x in embedding)}]\")\n",
    "\n",
    "print(\"\\n💡 Notice: Similar tokens have similar vector values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between token embeddings\n",
    "print(\"🔍 SIMILARITY ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Compare some pairs\n",
    "comparisons = [\n",
    "    (\"Hello\", \"Hi\"),           # Both greetings\n",
    "    (\"Hello\", \"Goodbye\"),      # Opposite meanings  \n",
    "    (\"world\", \"universe\"),     # Similar concepts\n",
    "    (\"Python\", \"programming\"), # Related concepts\n",
    "    (\"Hello\", \"Python\")        # Unrelated\n",
    "]\n",
    "\n",
    "for token1, token2 in comparisons:\n",
    "    emb1 = token_embeddings[token1].reshape(1, -1)\n",
    "    emb2 = token_embeddings[token2].reshape(1, -1)\n",
    "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
    "    \n",
    "    print(f\"'{token1}' ↔ '{token2}': {similarity:.3f}\")\n",
    "    if similarity > 0.8:\n",
    "        print(\"  → Very similar! 🎯\")\n",
    "    elif similarity > 0.3:\n",
    "        print(\"  → Somewhat similar\")\n",
    "    elif similarity < -0.3:\n",
    "        print(\"  → Opposite meanings! ↔️\")\n",
    "    else:\n",
    "        print(\"  → Not very related\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215557d1",
   "metadata": {},
   "source": [
    "## Real OpenAI Embeddings\n",
    "\n",
    "Now let's get real embeddings from OpenAI's API. These are the actual vectors that models like GPT-4 use!\n",
    "\n",
    "**Note**: You'll need an OpenAI API key. Set it as an environment variable or paste it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29381ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI API (you'll need your API key)\n",
    "# Option 1: Set environment variable OPENAI_API_KEY\n",
    "# Option 2: Uncomment and add your key below\n",
    "# openai.api_key = \"your-api-key-here\"\n",
    "\n",
    "# Check if API key is available\n",
    "try:\n",
    "    client = openai.OpenAI()  # Will use OPENAI_API_KEY env var\n",
    "    print(\"✅ OpenAI API key found!\")\n",
    "    api_available = True\n",
    "except:\n",
    "    print(\"❌ OpenAI API key not found.\")\n",
    "    print(\"Set OPENAI_API_KEY environment variable or uncomment the line above.\")\n",
    "    api_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b98489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real embeddings from OpenAI\n",
    "if api_available:\n",
    "    def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "        \"\"\"Get embedding for a text using OpenAI's API\"\"\"\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return np.array(response.data[0].embedding)\n",
    "    \n",
    "    # Test words for embedding analysis\n",
    "    test_words = [\n",
    "        \"king\", \"queen\", \"man\", \"woman\",\n",
    "        \"cat\", \"dog\", \"animal\",\n",
    "        \"happy\", \"joyful\", \"sad\",\n",
    "        \"Python\", \"programming\", \"code\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🌐 GETTING REAL OPENAI EMBEDDINGS...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    real_embeddings = {}\n",
    "    for word in test_words:\n",
    "        try:\n",
    "            embedding = get_embedding(word)\n",
    "            real_embeddings[word] = embedding\n",
    "            print(f\"✅ '{word}': {len(embedding)}D vector\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error getting embedding for '{word}': {e}\")\n",
    "            break\n",
    "    \n",
    "    if real_embeddings:\n",
    "        sample_word = list(real_embeddings.keys())[0]\n",
    "        sample_embedding = real_embeddings[sample_word]\n",
    "        print(f\"\\n📊 Embedding details:\")\n",
    "        print(f\"Dimensions: {len(sample_embedding)}\")\n",
    "        print(f\"Sample values for '{sample_word}': [{', '.join(f'{x:.4f}' for x in sample_embedding[:5])}...]\")\n",
    "        print(f\"Value range: {sample_embedding.min():.4f} to {sample_embedding.max():.4f}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping real embeddings - API key not available\")\n",
    "    print(\"We'll continue with simulated examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4de803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze real embedding similarities\n",
    "if api_available and real_embeddings:\n",
    "    print(\"🔍 REAL EMBEDDING SIMILARITIES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Interesting word pairs to compare\n",
    "    word_pairs = [\n",
    "        (\"king\", \"queen\"),        # Gender relationship\n",
    "        (\"man\", \"woman\"),         # Gender relationship\n",
    "        (\"cat\", \"dog\"),           # Both animals\n",
    "        (\"happy\", \"joyful\"),      # Synonyms\n",
    "        (\"happy\", \"sad\"),         # Opposites\n",
    "        (\"Python\", \"programming\"), # Related concepts\n",
    "        (\"cat\", \"programming\"),   # Unrelated\n",
    "    ]\n",
    "    \n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in real_embeddings and word2 in real_embeddings:\n",
    "            emb1 = real_embeddings[word1].reshape(1, -1)\n",
    "            emb2 = real_embeddings[word2].reshape(1, -1)\n",
    "            similarity = cosine_similarity(emb1, emb2)[0][0]\n",
    "            \n",
    "            print(f\"'{word1}' ↔ '{word2}': {similarity:.4f}\")\n",
    "            \n",
    "            # Interpret the similarity\n",
    "            if similarity > 0.8:\n",
    "                print(\"  → Extremely similar! 🎯\")\n",
    "            elif similarity > 0.6:\n",
    "                print(\"  → Very similar! ✨\")\n",
    "            elif similarity > 0.4:\n",
    "                print(\"  → Moderately similar 📊\")\n",
    "            elif similarity > 0.2:\n",
    "                print(\"  → Somewhat similar 🔍\")\n",
    "            else:\n",
    "                print(\"  → Not very similar 🔀\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"⚠️ Skipping real similarity analysis - embeddings not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7c94f",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "Since embeddings are high-dimensional (1536D), we can't visualize them directly. But we can use **dimensionality reduction** to project them into 2D space for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab567f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using PCA (Principal Component Analysis)\n",
    "if api_available and real_embeddings and len(real_embeddings) > 2:\n",
    "    print(\"📊 VISUALIZING EMBEDDINGS IN 2D:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Prepare data for PCA\n",
    "    words = list(real_embeddings.keys())\n",
    "    embeddings_matrix = np.array([real_embeddings[word] for word in words])\n",
    "    \n",
    "    # Reduce from 1536D to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings_matrix)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Define colors for different categories\n",
    "    colors = {\n",
    "        'royal': ['king', 'queen'],\n",
    "        'gender': ['man', 'woman'], \n",
    "        'animals': ['cat', 'dog', 'animal'],\n",
    "        'emotions': ['happy', 'joyful', 'sad'],\n",
    "        'tech': ['Python', 'programming', 'code']\n",
    "    }\n",
    "    \n",
    "    color_map = {}\n",
    "    color_list = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, (category, word_list) in enumerate(colors.items()):\n",
    "        for word in word_list:\n",
    "            if word in words:\n",
    "                color_map[word] = color_list[i % len(color_list)]\n",
    "    \n",
    "    # Plot points\n",
    "    for i, word in enumerate(words):\n",
    "        x, y = embeddings_2d[i]\n",
    "        color = color_map.get(word, 'gray')\n",
    "        plt.scatter(x, y, c=color, s=100, alpha=0.7)\n",
    "        plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    plt.title('OpenAI Embeddings Visualized in 2D Space\\n(Using PCA Dimensionality Reduction)', fontsize=14)\n",
    "    plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    for i, (category, word_list) in enumerate(colors.items()):\n",
    "        plt.scatter([], [], c=color_list[i % len(color_list)], label=category.capitalize(), s=100, alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n💡 Interpretation:\")\n",
    "    print(f\"- Words that are close together have similar meanings\")\n",
    "    print(f\"- The PCA captures {pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1]:.1%} of the variance\")\n",
    "    print(f\"- This is a 2D projection of {embeddings_matrix.shape[1]}D space!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Skipping visualization - need embeddings from OpenAI API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bfc3f",
   "metadata": {},
   "source": [
    "## How Embeddings Work in Language Models\n",
    "\n",
    "### The Complete Pipeline:\n",
    "\n",
    "1. **Text Input**: \"Hello world\"\n",
    "2. **Tokenization**: [9906, 1917] (from notebook 01)\n",
    "3. **Embedding Lookup**: Each token → 1536D vector\n",
    "4. **Model Processing**: Attention, transformations, etc.\n",
    "5. **Output**: Generated text, classifications, etc.\n",
    "\n",
    "### Key Insights:\n",
    "- **Fixed Size**: Every token gets the same size vector (1536D for OpenAI)\n",
    "- **Learned**: Embeddings are learned during model training\n",
    "- **Semantic**: Similar tokens have similar embeddings\n",
    "- **Context-Independent**: Each token has one embedding (context comes later in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the complete token-to-embedding pipeline\n",
    "print(\"🔄 COMPLETE PIPELINE DEMONSTRATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_text = \"Hello beautiful world\"\n",
    "print(f\"📝 Input Text: '{sample_text}'\")\n",
    "print()\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = encoding.encode(sample_text)\n",
    "print(f\"🔤 Step 1 - Tokenization:\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "for i, token in enumerate(tokens):\n",
    "    piece = encoding.decode([token])\n",
    "    print(f\"  Token {i+1}: {token} → '{piece}'\")\n",
    "print()\n",
    "\n",
    "# Step 2: Embedding lookup (simulated)\n",
    "print(f\"🧮 Step 2 - Embedding Lookup:\")\n",
    "print(f\"  Each token gets converted to a 1536D vector\")\n",
    "if api_available:\n",
    "    try:\n",
    "        full_embedding = get_embedding(sample_text)\n",
    "        print(f\"  Full text embedding shape: {full_embedding.shape}\")\n",
    "        print(f\"  Sample values: [{', '.join(f'{x:.4f}' for x in full_embedding[:5])}...]\")\n",
    "    except:\n",
    "        print(\"  (Simulated - would be 1536D vectors)\")\n",
    "else:\n",
    "    print(\"  (Simulated - would be 1536D vectors)\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        piece = encoding.decode([token])\n",
    "        print(f\"  '{piece}' → [1536 dimensional vector]\")\n",
    "\n",
    "print()\n",
    "print(f\"📊 Result: {len(tokens)} tokens → {len(tokens)} embeddings → Ready for model processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a98e1",
   "metadata": {},
   "source": [
    "## Next Steps & Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "1. **Tokens → Embeddings**: Each token becomes a fixed-size vector\n",
    "2. **Semantic Similarity**: Similar tokens have similar embeddings\n",
    "3. **High Dimensional**: Real embeddings are 1536D (much richer than our examples)\n",
    "4. **Foundation**: Embeddings are the foundation for all model processing\n",
    "\n",
    "### The Journey So Far:\n",
    "- **Notebook 01**: Text → Tokens (discrete IDs)\n",
    "- **Notebook 02**: Tokens → Embeddings (dense vectors)\n",
    "- **Next**: How models process these embeddings (attention, transformers, etc.)\n",
    "\n",
    "### Key Questions Answered:\n",
    "- ✅ How do models convert tokens to numbers they can work with?\n",
    "- ✅ Why do similar words have similar representations?\n",
    "- ✅ What does \"fixed-length\" mean in the context of variable-length text?\n",
    "\n",
    "### Experiment Ideas:\n",
    "1. Try getting embeddings for different languages\n",
    "2. Explore embeddings for code vs. natural language\n",
    "3. Test how embeddings change with context (spoiler: they don't at this stage!)\n",
    "4. Calculate embedding similarity for your own word pairs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
